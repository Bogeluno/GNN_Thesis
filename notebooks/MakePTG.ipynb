{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import date, timedelta\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric import utils, data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = [int(z[3:]) for z in pd.read_csv('data/processed/SimpleNNData.csv', index_col=0).filter(regex = 'lz').columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_PTG(graph, zones):\n",
    "    attr, adj = graph\n",
    "\n",
    "    # Filter out \n",
    "    if (attr.time_to_reservation.values[-1] >= 48) or ~attr.next_customer[-1]:\n",
    "        return None\n",
    "    \n",
    "    if attr.leave_zone[-1] not in zones:\n",
    "        return None\n",
    "\n",
    "    # Slice\n",
    "    _, labels = sparse.csgraph.connected_components(csgraph=adj, directed=False, return_labels=True)\n",
    "    newl = labels[-1]\n",
    "    indices = labels == newl   \n",
    "\n",
    "    attr = attr[indices]\n",
    "    adj = adj[indices,:].tocsc()[:,indices].tocsr()\n",
    "\n",
    "    # Time variables\n",
    "    attr['weekend'] = attr.time.dt.weekday//5\n",
    "\n",
    "    def circle_transform(col, max_val=86400):\n",
    "        tot_sec = ((col - col.dt.normalize()) / pd.Timedelta('1 second')).astype(int)\n",
    "        cos_val = np.cos(2*np.pi*tot_sec/max_val)\n",
    "        sin_val = np.sin(2*np.pi*tot_sec/max_val)\n",
    "        return cos_val, sin_val\n",
    "\n",
    "    attr['Time_Cos'], attr['Time_Sin'] = [x.values for x in circle_transform(attr.time)]\n",
    "\n",
    "    # drop\n",
    "    attr.drop(columns=['park_location_lat', 'park_location_long', 'leave_location_lat', 'leave_location_long', 'park_fuel', 'park_zone', 'moved', 'movedTF', 'time', 'prev_customer', 'next_customer'], inplace = True)\n",
    "\n",
    "    # One hot encoding\n",
    "    attr['leave_zone'] = pd.Categorical(attr['leave_zone'], categories=zones)\n",
    "    attr = pd.get_dummies(attr, columns = ['leave_zone'], prefix='lz')\n",
    "\n",
    "    attr['engine']= pd.Categorical(attr['engine'], categories=['118I', 'I3', 'COOPER', 'X1'])\n",
    "    attr = pd.get_dummies(attr, columns = ['engine'], prefix='eng')\n",
    "\n",
    "    # Normalize fuel and dist \n",
    "    attr['leave_fuel'] = attr['leave_fuel']/100\n",
    "    #df['dist_to_station'] = df['dist_to_station']/5320\n",
    "\n",
    "    # Get edges\n",
    "    edge_index, edge_weight = utils.convert.from_scipy_sparse_matrix(adj)\n",
    "\n",
    "    # Make pytorch data type\n",
    "    d = data.Data(x = torch.tensor(attr.drop(columns = ['time_to_reservation']).to_numpy(dtype = 'float')).float(), edge_index=edge_index, edge_attr=edge_weight.float(), y = torch.tensor(attr.time_to_reservation.values).float())\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['data/processed/Graphs/'+(sdate + timedelta(days=i)).strftime(\"%Y%m%d\")+'.pickle' for i in range(delta.days + 1)]\n",
    "with open(files[0], 'rb') as f:\n",
    "    graph_collection = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = next(iter(graph_collection.values()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time variables\n",
    "attr['weekend'] = attr.time.dt.weekday//5\n",
    "\n",
    "def circle_transform(col, max_val=86400):\n",
    "    tot_sec = ((col - col.dt.normalize()) / pd.Timedelta('1 second')).astype(int)\n",
    "    cos_val = np.cos(2*np.pi*tot_sec/max_val)\n",
    "    sin_val = np.sin(2*np.pi*tot_sec/max_val)\n",
    "    return cos_val, sin_val\n",
    "\n",
    "attr['Time_Cos'], attr['Time_Sin'] = [x.values for x in circle_transform(attr.time)]\n",
    "\n",
    "# drop\n",
    "attr.drop(columns=['park_location_lat', 'park_location_long', 'leave_location_lat', 'leave_location_long', 'park_fuel', 'park_zone', 'moved', 'movedTF', 'time', 'prev_customer', 'next_customer'], inplace = True)\n",
    "\n",
    "# One hot encoding\n",
    "attr['leave_zone'] = pd.Categorical(attr['leave_zone'], categories=zones)\n",
    "attr = pd.get_dummies(attr, columns = ['leave_zone'], prefix='lz')\n",
    "\n",
    "attr['engine']= pd.Categorical(attr['engine'], categories=['118I', 'I3', 'COOPER', 'X1'])\n",
    "attr = pd.get_dummies(attr, columns = ['engine'], prefix='eng')\n",
    "\n",
    "# Normalize fuel and dist \n",
    "attr['leave_fuel'] = attr['leave_fuel']/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>leave_fuel</th>\n",
       "      <th>weekend</th>\n",
       "      <th>Time_Cos</th>\n",
       "      <th>Time_Sin</th>\n",
       "      <th>lz_102111</th>\n",
       "      <th>lz_102121</th>\n",
       "      <th>lz_102122</th>\n",
       "      <th>lz_102131</th>\n",
       "      <th>lz_102141</th>\n",
       "      <th>lz_102142</th>\n",
       "      <th>...</th>\n",
       "      <th>lz_185132</th>\n",
       "      <th>lz_185141</th>\n",
       "      <th>lz_185142</th>\n",
       "      <th>lz_185143</th>\n",
       "      <th>lz_185154</th>\n",
       "      <th>lz_185203</th>\n",
       "      <th>eng_118I</th>\n",
       "      <th>eng_I3</th>\n",
       "      <th>eng_COOPER</th>\n",
       "      <th>eng_X1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>WBA1R5108J5K57642</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.942908</td>\n",
       "      <td>0.333053</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WMWXU7106KTM90596</th>\n",
       "      <td>0.82</td>\n",
       "      <td>1</td>\n",
       "      <td>0.793486</td>\n",
       "      <td>0.608588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WMWXR3103KTK68690</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.837957</td>\n",
       "      <td>0.545736</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WBY8P2107K7D92186</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.438567</td>\n",
       "      <td>0.898698</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WBY8P210XK7D96376</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.497416</td>\n",
       "      <td>0.867512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WBY8P2104K7D98348</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>-0.010035</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WMWXU710XKTM91735</th>\n",
       "      <td>0.57</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>-0.008872</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WMWXU7106KTM91411</th>\n",
       "      <td>0.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.008727</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WBY8P2103K7D77216</th>\n",
       "      <td>0.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>-0.005163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WBY1Z21050V308285</th>\n",
       "      <td>0.77</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403 rows × 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   leave_fuel  weekend  Time_Cos  Time_Sin  lz_102111  \\\n",
       "car                                                                     \n",
       "WBA1R5108J5K57642        0.36        0  0.942908  0.333053          0   \n",
       "WMWXU7106KTM90596        0.82        1  0.793486  0.608588          0   \n",
       "WMWXR3103KTK68690        0.28        0  0.837957  0.545736          0   \n",
       "WBY8P2107K7D92186        0.56        0 -0.438567  0.898698          0   \n",
       "WBY8P210XK7D96376        0.58        0 -0.497416  0.867512          0   \n",
       "...                       ...      ...       ...       ...        ...   \n",
       "WBY8P2104K7D98348        1.00        1  0.999950 -0.010035          0   \n",
       "WMWXU710XKTM91735        0.57        1  0.999961 -0.008872          0   \n",
       "WMWXU7106KTM91411        0.55        1  0.999962 -0.008727          0   \n",
       "WBY8P2103K7D77216        0.66        1  0.999987 -0.005163          0   \n",
       "WBY1Z21050V308285        0.77        1  0.999990  0.004436          0   \n",
       "\n",
       "                   lz_102121  lz_102122  lz_102131  lz_102141  lz_102142  ...  \\\n",
       "car                                                                       ...   \n",
       "WBA1R5108J5K57642          0          0          0          0          0  ...   \n",
       "WMWXU7106KTM90596          0          0          0          0          0  ...   \n",
       "WMWXR3103KTK68690          0          0          0          0          0  ...   \n",
       "WBY8P2107K7D92186          0          0          0          0          0  ...   \n",
       "WBY8P210XK7D96376          0          0          0          0          0  ...   \n",
       "...                      ...        ...        ...        ...        ...  ...   \n",
       "WBY8P2104K7D98348          0          0          0          0          0  ...   \n",
       "WMWXU710XKTM91735          0          0          0          0          0  ...   \n",
       "WMWXU7106KTM91411          0          0          0          0          0  ...   \n",
       "WBY8P2103K7D77216          0          0          0          0          0  ...   \n",
       "WBY1Z21050V308285          0          0          0          0          0  ...   \n",
       "\n",
       "                   lz_185132  lz_185141  lz_185142  lz_185143  lz_185154  \\\n",
       "car                                                                        \n",
       "WBA1R5108J5K57642          0          0          0          0          1   \n",
       "WMWXU7106KTM90596          0          0          0          0          1   \n",
       "WMWXR3103KTK68690          0          0          0          0          1   \n",
       "WBY8P2107K7D92186          0          0          0          0          0   \n",
       "WBY8P210XK7D96376          0          0          0          0          0   \n",
       "...                      ...        ...        ...        ...        ...   \n",
       "WBY8P2104K7D98348          0          0          0          0          0   \n",
       "WMWXU710XKTM91735          0          0          0          0          0   \n",
       "WMWXU7106KTM91411          0          0          0          0          0   \n",
       "WBY8P2103K7D77216          0          0          0          0          0   \n",
       "WBY1Z21050V308285          0          0          0          0          0   \n",
       "\n",
       "                   lz_185203  eng_118I  eng_I3  eng_COOPER  eng_X1  \n",
       "car                                                                 \n",
       "WBA1R5108J5K57642          0         1       0           0       0  \n",
       "WMWXU7106KTM90596          0         0       0           1       0  \n",
       "WMWXR3103KTK68690          0         0       0           1       0  \n",
       "WBY8P2107K7D92186          0         0       1           0       0  \n",
       "WBY8P210XK7D96376          0         0       1           0       0  \n",
       "...                      ...       ...     ...         ...     ...  \n",
       "WBY8P2104K7D98348          0         0       1           0       0  \n",
       "WMWXU710XKTM91735          0         0       0           1       0  \n",
       "WMWXU7106KTM91411          0         0       0           1       0  \n",
       "WBY8P2103K7D77216          1         0       1           0       0  \n",
       "WBY1Z21050V308285          0         0       1           0       0  \n",
       "\n",
       "[403 rows x 264 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr.drop(columns = ['time_to_reservation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:59<00:00, 29.99s/it]\n"
     ]
    }
   ],
   "source": [
    "sdate = date(2019, 9, 15)   # start date\n",
    "edate = date(2019, 9, 17)   # end date\n",
    "delta = edate - sdate       # as timedelta\n",
    "files = ['data/processed/Graphs/'+(sdate + timedelta(days=i)).strftime(\"%Y%m%d\")+'.pickle' for i in range(delta.days + 1)]\n",
    "\n",
    "dataset = []\n",
    "\n",
    "with open(files[0], 'rb') as f:\n",
    "    graph_collection = pickle.load(f)\n",
    "\n",
    "for g in graph_collection.values():\n",
    "    res = make_PTG(g,zones)\n",
    "    if res:\n",
    "        dataset.append(res)\n",
    "\n",
    "train_val_size = int(0.8 * len(dataset))\n",
    "val_test_size = len(dataset)-train_val_size\n",
    "train_val_data, test_data = torch.utils.data.random_split(dataset, [train_val_size, val_test_size])\n",
    "train_size = train_val_size-val_test_size\n",
    "train_data, val_data = torch.utils.data.random_split(train_val_data, [train_size, val_test_size])\n",
    "del train_val_data\n",
    "\n",
    "for file in tqdm(files[1:]):\n",
    "    dataset = []\n",
    "    with open(file, 'rb') as f:\n",
    "        graph_collection = pickle.load(f)\n",
    "\n",
    "    for g in graph_collection.values():\n",
    "        res = make_PTG(g,zones)\n",
    "        if res:\n",
    "            dataset.append(res)\n",
    "\n",
    "    train_val_size = int(0.8 * len(dataset))\n",
    "    val_test_size = len(dataset)-train_val_size\n",
    "    train_val_data_tmp, test_data_tmp = torch.utils.data.random_split(dataset, [train_val_size, val_test_size])\n",
    "    train_size = train_val_size-val_test_size\n",
    "    train_data_tmp, val_data_tmp = torch.utils.data.random_split(train_val_data_tmp, [train_size, val_test_size])\n",
    "\n",
    "    train_data = torch.utils.data.ConcatDataset([train_data,train_data_tmp])\n",
    "    val_data = torch.utils.data.ConcatDataset([val_data,val_data_tmp])\n",
    "    test_data = torch.utils.data.ConcatDataset([test_data,test_data_tmp])\n",
    "\n",
    "#del train_val_data_tmp, test_data_tmp, train_data_tmp, val_data_tmp, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.ConcatDataset at 0x7fbde8aefb80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:43<00:00, 37.21s/it]\n"
     ]
    }
   ],
   "source": [
    "sdate = date(2019, 8, 15)   # start date\n",
    "edate = date(2019, 8, 20)   # end date\n",
    "delta = edate - sdate       # as timedelta\n",
    "files = ['data/processed/Graphs/'+(sdate + timedelta(days=i)).strftime(\"%Y%m%d\")+'.pickle' for i in range(delta.days + 1)]\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    with open(file, 'rb') as f:\n",
    "        graph_collection = pickle.load(f)\n",
    "\n",
    "    for g in graph_collection.values():\n",
    "        res = make_PTG(g,zones)\n",
    "        if res:\n",
    "            dataset.append(res.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_size = int(0.8 * len(dataset))\n",
    "val_test_size = len(dataset)-train_val_size\n",
    "train_val_data, test_data = torch.utils.data.random_split(dataset, [train_val_size, val_test_size])\n",
    "train_size = train_val_size-val_test_size\n",
    "train_data, val_data = torch.utils.data.random_split(train_val_data, [train_size, val_test_size])\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)//32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3600,  0.0000,  0.9429,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8200,  1.0000,  0.7935,  ...,  0.0000,  1.0000,  0.0000],\n",
       "        [ 0.2800,  0.0000,  0.8380,  ...,  0.0000,  1.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 1.0000,  1.0000, -0.7238,  ...,  1.0000,  0.0000,  0.0000],\n",
       "        [ 0.5400,  1.0000, -0.7267,  ...,  1.0000,  0.0000,  0.0000],\n",
       "        [ 0.4300,  1.0000, -0.7269,  ...,  1.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter\n",
    "def pool_neighbor(x, edge_index, num_nodes, reduce = 'mean'):\n",
    "    r\"\"\"Average pools neighboring node features, where each feature in\n",
    "    :obj:`data.x` is replaced by the average feature values from the central\n",
    "    node and its neighbors.\n",
    "    \"\"\"\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = (row, col)\n",
    "\n",
    "    x = scatter(x[row], col, dim=0, dim_size=num_nodes, reduce=reduce)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(269, 32)\n",
      "  (conv2): GCNConv(32, 1)\n",
      ") 8673\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(269, 32)\n",
    "        self.conv2 = GCNConv(32, 1)\n",
    "        #self.conv3 = GCNConv(32, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p = 0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.conv3(x, edge_index, edge_weight)\n",
    "\n",
    "        return x.squeeze()\n",
    "\n",
    "GNN = GCN().to(device)\n",
    "print(GNN, sum(p.numel() for p in GNN.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(GNN.parameters(), lr=0.03, weight_decay = 0.0001) #Chaged to Adam and learning + regulariztion rate set\n",
    "criterion = nn.MSELoss(reduction = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Train Loss 18.009013 , Valid Loss 15.885797 ,Train R2 0.207517, Valid R2 0.165569\n",
      "Epoch  2: Train Loss 17.043701 , Valid Loss 15.617674 ,Train R2 0.226370, Valid R2 0.179597\n",
      "Epoch  3: Train Loss 16.611401 , Valid Loss 15.763800 ,Train R2 0.245041, Valid R2 0.171937\n"
     ]
    }
   ],
   "source": [
    "# Set number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "# Set up lists for loss/R2\n",
    "train_r2, train_loss = [], []\n",
    "valid_r2, valid_loss = [], []\n",
    "cur_loss = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ### Train\n",
    "    cur_loss_train = []\n",
    "    GNN.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = GNN(batch)\n",
    "        batch_loss = criterion(out[batch.ptr[1:]-1],batch.y[batch.ptr[1:]-1])\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cur_loss_train.append(batch_loss.item())\n",
    "    \n",
    "    train_losses.append(np.mean(cur_loss_train))\n",
    "\n",
    "    ### Evaluate training\n",
    "    GNN.eval()\n",
    "    train_preds, train_targs = [], []\n",
    "    for batch in train_loader:\n",
    "        preds = GNN(batch)\n",
    "        train_targs += list(batch.y.numpy()[batch.ptr[1:]-1])\n",
    "        train_preds += list(preds.detach().numpy()[batch.ptr[1:]-1])\n",
    "\n",
    "\n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    cur_loss_val = []\n",
    "    for batch in val_loader:\n",
    "        preds = GNN(batch)[batch.ptr[1:]-1]\n",
    "        y_val = batch.y[batch.ptr[1:]-1]\n",
    "        val_targs += list(y_val.numpy())\n",
    "        val_preds += list(preds.detach().numpy())\n",
    "        cur_loss_val.append(criterion(preds, y_val).item())\n",
    "\n",
    "    val_losses.append(np.mean(cur_loss_val))\n",
    "\n",
    "\n",
    "    train_r2_cur = r2_score(train_targs, train_preds)\n",
    "    valid_r2_cur = r2_score(val_targs, val_preds)\n",
    "    \n",
    "    train_r2.append(train_r2_cur)\n",
    "    valid_r2.append(valid_r2_cur)\n",
    "\n",
    "    print(\"Epoch %2i: Train Loss %f , Valid Loss %f ,Train R2 %f, Valid R2 %f\" % (\n",
    "                epoch+1, train_losses[-1], val_losses[-1],train_r2_cur, valid_r2_cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3z/pggbcw7949507j0lbw1gjngc0000gn/T/ipykernel_13599/3107082710.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4084\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m121\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnum_samples_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnum_batches_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples_train\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_samples_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# setting hyperparameters and gettings epoch sizes\n",
    "batch_size = 4084\n",
    "num_epochs = 121\n",
    "num_samples_train = X_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = X_val.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_r2, train_loss = [], []\n",
    "valid_r2, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10, verbose=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss_train = []\n",
    "    net.train()\n",
    "    for i in range(num_batches_train):\n",
    "        optimizer.zero_grad()\n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = net(X_train[slce])\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        target_batch = y_train[slce]\n",
    "        batch_loss = criterion(output, target_batch)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cur_loss_train.append(batch_loss.item())\n",
    "    train_losses.append(np.mean(cur_loss_train))\n",
    "\n",
    "    net.eval()\n",
    "    ### Evaluate training\n",
    "    train_preds, train_targs = [], []\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = net(X_train[slce])\n",
    "        \n",
    "        preds = output\n",
    "        \n",
    "        train_targs += list(y_train[slce].numpy())\n",
    "        train_preds += list(preds.data.numpy())\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    cur_loss_val = []\n",
    "    for i in range(num_batches_valid):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        \n",
    "        output = net(X_val[slce])\n",
    "        preds = output\n",
    "        val_targs += list(y_val[slce].numpy())\n",
    "        val_preds += list(preds.data.numpy())\n",
    "\n",
    "        cur_loss_val.append(criterion(output, y_val[slce]).item())\n",
    "\n",
    "    val_losses.append(np.mean(cur_loss_val))\n",
    "\n",
    "\n",
    "    train_r2_cur = r2_score(train_targs, train_preds)\n",
    "    valid_r2_cur = r2_score(val_targs, val_preds)\n",
    "    \n",
    "    train_r2.append(train_r2_cur)\n",
    "    valid_r2.append(valid_r2_cur)\n",
    "\n",
    "    # EarlyStopping\n",
    "    early_stopping(val_losses[-1], net)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        print(\"Epoch %2i: Train Loss %f , Valid Loss %f , Train R2 %f, Valid R2 %f\" % (\n",
    "            epoch+1, train_losses[-1], val_losses[-1], train_r2_cur, valid_r2_cur))\n",
    "        break\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %2i: Train Loss %f , Valid Loss %f ,Train R2 %f, Valid R2 %f\" % (\n",
    "                epoch+1, train_losses[-1], val_losses[-1],train_r2_cur, valid_r2_cur))\n",
    "\n",
    "epoch = np.arange(len(train_r2))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_r2, 'r', epoch, valid_r2, 'b')\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('R2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get files\n",
    "sdate = date(2019, 8, 15)   # start date\n",
    "edate = date(2019, 10, 2)   # end date\n",
    "delta = edate - sdate       # as timedelta\n",
    "files = ['graphs/'+(sdate + timedelta(days=i)).strftime(\"%Y%m%d\")+'.pickle' for i in range(delta.days + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0815'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0][11:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5509f31e2044e4d39f528bdd5e1bbe95ee9581822c64151f31115901a677525b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('GNN_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
