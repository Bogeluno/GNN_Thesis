{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch_sparse/_convert_cpu.so, 6): Symbol not found: __ZN2at5emptyEN3c108ArrayRefIxEENS0_13TensorOptionsENS0_8optionalINS0_12MemoryFormatEEE\n  Referenced from: /opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch_sparse/_convert_cpu.so\n  Expected in: /opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n in /opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch_sparse/_convert_cpu.so",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3z/pggbcw7949507j0lbw1gjngc0000gn/T/ipykernel_37703/513727762.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhaversine_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcar2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhetero_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m from typing import (Optional, Dict, Any, Union, List, Iterable, Tuple,\n\u001b[1;32m      2\u001b[0m                     NamedTuple, Callable)\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch_geometric/typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Types for accessing data ####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch_sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;34m'_relabel'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m ]:\n\u001b[0;32m---> 15\u001b[0;31m     torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n\u001b[0m\u001b[1;32m     16\u001b[0m         f'{library}_{suffix}', [osp.dirname(__file__)]).origin)\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/GNN_env/lib/python3.9/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch_sparse/_convert_cpu.so, 6): Symbol not found: __ZN2at5emptyEN3c108ArrayRefIxEENS0_13TensorOptionsENS0_8optionalINS0_12MemoryFormatEEE\n  Referenced from: /opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch_sparse/_convert_cpu.so\n  Expected in: /opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n in /opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/torch_sparse/_convert_cpu.so"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import folium\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch_geometric import utils, data\n",
    "\n",
    "def haversine_start(df, car1, car2, max_dist = 1500):\n",
    "    def _edge_weight(x, max_dist):\n",
    "        return max((max_dist-x)/max_dist,0)\n",
    "    dfc1 = df[df.car == car1]\n",
    "    dfc2 = df[df.car == car2]\n",
    "\n",
    "    point1 = dfc1[['leave_location_lat','leave_location_long']].values[0]\n",
    "    point2 = dfc2[['leave_location_lat','leave_location_long']].values[0]\n",
    "    #return point1\n",
    "\n",
    "    # convert decimal degrees to radians\n",
    "    lat1, lon1 = map(np.radians, point1)\n",
    "    lat2, lon2 = map(np.radians, point2)\n",
    "\n",
    "    # Deltas\n",
    "    delta_lon = lon2 - lon1 \n",
    "    delta_lat = lat2 - lat1 \n",
    "    \n",
    "    # haversine formula \n",
    "    a = np.sin(delta_lat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(delta_lon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371000 # Radius of earth in m\n",
    "    return _edge_weight(c * r, max_dist)\n",
    "\n",
    "def haversine_add(current_locs, to_add, max_dist = 1500):\n",
    "    def _edge_weight(x, max_dist):\n",
    "        return max((max_dist-x)/max_dist,0)\n",
    "    def _haversine(point1, lat2, lon2):\n",
    "        \n",
    "         # convert decimal degrees to radians\n",
    "        lat1, lon1 = map(np.radians, point1)\n",
    "\n",
    "        # Deltas\n",
    "        delta_lon = lon2 - lon1 \n",
    "        delta_lat = lat2 - lat1 \n",
    "        \n",
    "        # haversine formula \n",
    "        a = np.sin(delta_lat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(delta_lon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a)) \n",
    "        r = 6371000 # Radius of earth in m\n",
    "        return c * r\n",
    "\n",
    "    leave_lat_add, leave_long_add = to_add.leave_location_lat, to_add.leave_location_long\n",
    "\n",
    "    lat2, lon2 = map(np.radians, [leave_lat_add,leave_long_add])\n",
    "\n",
    "    new_weights = [_edge_weight(_haversine(loc, lat2, lon2), max_dist) for loc in current_locs]\n",
    "\n",
    "    return new_weights\n",
    "\n",
    "\n",
    "def delete_rc(mat, i):\n",
    "    # row\n",
    "    n = mat.indptr[i+1] - mat.indptr[i]\n",
    "    if n > 0:\n",
    "        mat.data[mat.indptr[i]:-n] = mat.data[mat.indptr[i+1]:]\n",
    "        mat.data = mat.data[:-n]\n",
    "        mat.indices[mat.indptr[i]:-n] = mat.indices[mat.indptr[i+1]:]\n",
    "        mat.indices = mat.indices[:-n]\n",
    "    mat.indptr[i:-1] = mat.indptr[i+1:]\n",
    "    mat.indptr[i:] -= n\n",
    "    mat.indptr = mat.indptr[:-1]\n",
    "    mat._shape = (mat._shape[0]-1, mat._shape[1])\n",
    "\n",
    "    # col\n",
    "    mat = mat.tocsc()\n",
    "    n = mat.indptr[i+1] - mat.indptr[i]\n",
    "    if n > 0:\n",
    "        mat.data[mat.indptr[i]:-n] = mat.data[mat.indptr[i+1]:]\n",
    "        mat.data = mat.data[:-n]\n",
    "        mat.indices[mat.indptr[i]:-n] = mat.indices[mat.indptr[i+1]:]\n",
    "        mat.indices = mat.indices[:-n]\n",
    "    mat.indptr[i:-1] = mat.indptr[i+1:]\n",
    "    mat.indptr[i:] -= n\n",
    "    mat.indptr = mat.indptr[:-1]\n",
    "    mat._shape = (mat._shape[0], mat._shape[1]-1)\n",
    "\n",
    "    return mat.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3401726 entries, 0 to 3401725\n",
      "Data columns (total 17 columns):\n",
      " #   Column               Dtype         \n",
      "---  ------               -----         \n",
      " 0   car                  object        \n",
      " 1   time                 datetime64[ns]\n",
      " 2   time_to_reservation  float32       \n",
      " 3   park_location_lat    float32       \n",
      " 4   park_location_long   float32       \n",
      " 5   leave_location_lat   float32       \n",
      " 6   leave_location_long  float32       \n",
      " 7   park_zone            int32         \n",
      " 8   leave_zone           int32         \n",
      " 9   park_fuel            int8          \n",
      " 10  leave_fuel           int8          \n",
      " 11  engine               object        \n",
      " 12  moved                float32       \n",
      " 13  prev_customer        bool          \n",
      " 14  next_customer        bool          \n",
      " 15  movedTF              bool          \n",
      " 16  action               bool          \n",
      "dtypes: bool(4), datetime64[ns](1), float32(6), int32(2), int8(2), object(2)\n",
      "memory usage: 227.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/processed/VacancySplit.csv', index_col=0, parse_dates = [2]).astype({'time_to_reservation': 'float32', 'park_location_lat': 'float32', 'park_location_long': 'float32', 'leave_location_lat': 'float32', 'leave_location_long': 'float32', 'park_zone': 'int32', 'leave_zone': 'int32', 'park_fuel': 'int8', 'leave_fuel': 'int8', 'moved': 'float32', 'movedTF': 'bool'})\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start_Time = pd.Timestamp('2018-07-09 14:27:00')\n",
    "start_df = df[df.time <= Start_Time]\n",
    "propegate_df = df[df.time > Start_Time]\n",
    "\n",
    "# Start\n",
    "CarID_dict_start = dict(iter(start_df.groupby('car')))\n",
    "Start_Garph_data = []\n",
    "\n",
    "for sub_df in CarID_dict_start.values():\n",
    "    last_obs = sub_df.iloc[-1]\n",
    "    if last_obs.action: # True is park\n",
    "        Start_Garph_data.append(last_obs)\n",
    "\n",
    "start_df_graph = pd.DataFrame(Start_Garph_data).iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 431/431 [04:33<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "max_dist = 1500\n",
    "def _edge_weight(x, max_dist):\n",
    "        return max((max_dist-x)/max_dist,0)\n",
    "A = pd.DataFrame(data = [[haversine_start(start_df_graph, car1, car2) for car1 in start_df_graph.car] for car2 in tqdm(start_df_graph.car)], index = start_df_graph.car, columns=start_df_graph.car, dtype='float16')\n",
    "\n",
    "# And make it sparse\n",
    "As = sparse.csr_matrix(A.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "As = sparse.csr_matrix(A.values)\n",
    "Graph_dict = {pd.Timestamp('2018-07-09 14:27:00'): (start_df_graph ,As)}\n",
    "node_data = start_df_graph.set_index('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6999/2800087 [02:05<13:55:48, 55.70it/s]\n"
     ]
    }
   ],
   "source": [
    "positive_time = propegate_df[propegate_df.action].time.diff().shift(-1) > pd.Timedelta(0,'s')\n",
    "positive_time[-1] = True\n",
    "\n",
    "new_day = (propegate_df.time.dt.date.diff().shift(-1) > pd.Timedelta(0,'s'))\n",
    "new_day[-1] = True\n",
    "\n",
    "i = 0\n",
    "for idx, next_row in tqdm(propegate_df.iterrows(), total = propegate_df.shape[0]):\n",
    "    if next_row.action: # True is park\n",
    "        # Get current locs\n",
    "        locs = [[attr['leave_location_lat'], attr['leave_location_long']] for _, attr in node_data.iterrows()]\n",
    "        \n",
    "        # Add to node data\n",
    "        node_data = node_data.append(next_row.rename(index = next_row['car']).iloc[1:16], verify_integrity = True)\n",
    "\n",
    "        # Calculate new weights\n",
    "        new_weights = haversine_add(locs, next_row, max_dist = 1500)\n",
    "\n",
    "        # Add new weights to adjacency\n",
    "        As = sparse.hstack([sparse.vstack([As,sparse.csr_matrix(new_weights)]).tocsc(), sparse.csc_matrix(new_weights+[1]).T]).tocsr()\n",
    "\n",
    "    else:\n",
    "        # Getindex\n",
    "        idx_to_drop = np.where(node_data.index == next_row.car)[0][0]\n",
    "\n",
    "        # Drop it\n",
    "        As = delete_rc(As, idx_to_drop)\n",
    "\n",
    "        # Drop from feature-matrix\n",
    "        node_data.drop(index = next_row.car, inplace=True)\n",
    "        \n",
    "    # Save graph if new time \n",
    "    if positive_time.get(idx):\n",
    "        Graph_dict[next_row.time] = (node_data.copy(), As.copy())\n",
    "\n",
    "    # Save file every day on last obs\n",
    "    if new_day[idx]:\n",
    "        f_name = next_row.time.strftime('%Y%m%d')+'.pickle'\n",
    "        with open(f_name, 'wb') as handle:\n",
    "            pickle.dump(Graph_dict, handle, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Clear memory\n",
    "        Graph_dict = {}\n",
    "\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    if i == 7000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To PTG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_zones = propegate_df.park_zone.unique()\n",
    "leave_zones = propegate_df.leave_zone.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/processed/zones.npy', 'wb') as f:\n",
    "    np.save(f, propegate_df.park_zone.unique())\n",
    "    np.save(f, propegate_df.leave_zone.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_PTG(graph, park_zones, leave_zones):\n",
    "    attr, adj = graph\n",
    "\n",
    "    # Slice\n",
    "    _, labels = connected_components(csgraph=adj, directed=False, return_labels=True)\n",
    "    newl = labels[-1]\n",
    "    indices = labels == newl   \n",
    "\n",
    "    attr = attr[indices]\n",
    "    adj = adj[indices,:].tocsc()[:,indices].tocsr()\n",
    "\n",
    "    # Binarize\n",
    "    attr[[\"prev_customer\", \"next_customer\"]] = attr[[\"prev_customer\", \"next_customer\"]].astype(int)\n",
    "\n",
    "    # One hot encoding\n",
    "    attr['park_zone'] = pd.Categorical(attr['park_zone'], categories=park_zones)\n",
    "    attr = pd.get_dummies(attr, columns= ['park_zone'], prefix='pz')\n",
    "\n",
    "    attr['leave_zone'] = pd.Categorical(attr['leave_zone'], categories=leave_zones)\n",
    "    attr = pd.get_dummies(attr, columns = ['leave_zone'], prefix='lz')\n",
    "\n",
    "    attr['engine']= pd.Categorical(attr['engine'], categories=['118I', 'I3', 'COOPER', 'X1'])\n",
    "    attr = pd.get_dummies(attr, columns = ['engine'], prefix='eng')\n",
    "\n",
    "    # Get edges\n",
    "    edge_index, edge_weight = utils.convert.from_scipy_sparse_matrix(adj)\n",
    "\n",
    "    # Make pytorch data type\n",
    "    d = data.Data(x = torch.tensor(attr.filter(regex = 'park_fuel|pz|lz|eng').values), edge_index=edge_index, edge_attr=edge_weight, y = torch.tensor(attr.time_to_reservation.values))\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1900/1900 [00:34<00:00, 55.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "loader = DataLoader([make_PTG(g,park_zones,leave_zones) for g in tqdm(tenth.values())], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   441,   883,  1324,  1764,  2205,  2646,  3088,  3530,  3972,\n",
       "         4415,  4859,  5302,  5745,  6189,  6633,  7073,  7514,  7954,  8394,\n",
       "         8834,  9274,  9715, 10155, 10594, 11034, 11474, 11915, 12357, 12800,\n",
       "        13244, 13688, 14130])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader)).ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (batch0): BatchNorm(366)\n",
      "  (conv1): GCNConv(366, 512)\n",
      "  (batch1): BatchNorm(512)\n",
      "  (conv2): GCNConv(512, 512)\n",
      "  (batch2): BatchNorm(512)\n",
      "  (conv3): GCNConv(512, 512)\n",
      "  (batch3): BatchNorm(512)\n",
      "  (conv4): GCNConv(512, 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv, TransformerConv, BatchNorm, SAGEConv, GNNExplainer\n",
    "import torch.nn.functional as F\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, ):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.batch0 = BatchNorm(num_features)\n",
    "\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.batch1 = BatchNorm(hidden_channels)\n",
    "\n",
    "        self.conv2 = GCNConv(hidden_channels, int(hidden_channels))\n",
    "        self.batch2 = BatchNorm(int(hidden_channels))\n",
    "\n",
    "        self.conv3 = GCNConv(int(hidden_channels), int(hidden_channels))\n",
    "        self.batch3 = BatchNorm(int(hidden_channels))\n",
    "        \n",
    "        self.conv4 = GCNConv(int(hidden_channels), 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.batch1(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.batch2(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.batch3(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "  \n",
    "        x = self.conv4(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(num_features=366, hidden_channels=512).to(device)\n",
    "#d.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "717533"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=5e-4, nesterov=True, momentum=0.9)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "def train():\n",
    "  model.train()\n",
    "  optimizer.zero_grad()  # Clear gradients.\n",
    "  out = model(data.x.to(torch.float), data.edge_index)  # Perform a single forward pass.\n",
    "  loss = criterion(out[data.train_mask], data.y[data.train_mask].to(torch.float))  # Compute the loss solely based on the training nodes.\n",
    "  loss.backward()  # Derive gradients.\n",
    "  optimizer.step()  # Update parameters based on gradients.\n",
    "  return loss\n",
    "\n",
    "def valid():\n",
    "  model.eval()\n",
    "  out = model(data.x.to(torch.float), data.edge_index) \n",
    "  loss = criterion(out[data.val_mask], data.y[data.val_mask].to(torch.float))\n",
    "  return loss\n",
    "\n",
    "for epoch in range(0, 2501):\n",
    "  train_loss = train().detach().cpu()\n",
    "  valid_loss = valid().detach().cpu()\n",
    "  train_loss_list.append(train_loss)\n",
    "  valid_loss_list.append(valid_loss)\n",
    "  if epoch % 100 == 0:\n",
    "      print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES', use_node_attr=True)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.datasets.tu_dataset.TUDataset"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5509f31e2044e4d39f528bdd5e1bbe95ee9581822c64151f31115901a677525b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('GNN_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
