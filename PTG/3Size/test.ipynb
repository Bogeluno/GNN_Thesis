{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import date, timedelta\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import Sequential, GCNConv, Linear\n",
    "from torch_geometric import utils, data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import r2_score, classification_report\n",
    "from torch_scatter import scatter\n",
    "import subprocess\n",
    "import time\n",
    "t = time.time()\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "no_days = 2\n",
    "print(device)\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 10\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.counter % 5 == 0:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def r2_loss(output, target):\n",
    "    target_mean = torch.mean(target)\n",
    "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
    "    ss_res = torch.sum((target - output) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return -r2\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# Load slicing\n",
    "with open(\"data/processed/Sample_NC\", \"rb\") as fp: \n",
    "    nc = pickle.load(fp)\n",
    "\n",
    "with open(\"data/processed/Sample_CC\", \"rb\") as fp: \n",
    "    cc = pickle.load(fp)\n",
    "\n",
    "# For classification\n",
    "df_full = pd.read_csv('data/processed/SimpleNNData.csv', index_col=0, parse_dates = [1]).sort_values(by = 'time')\n",
    "Clas_Coef = dict(pd.concat([df_full.time.dt.hour.iloc[np.concatenate(cc[:2])],df_full.time_to_reservation.iloc[np.concatenate(cc[:2])]], axis = 1).groupby('time')['time_to_reservation'].mean()*2)\n",
    "df_clas = pd.concat([df_full.time.dt.hour.iloc[cc[2]],df_full.time_to_reservation.iloc[cc[2]]], axis = 1)\n",
    "df_clas['Cut'] = df_clas.time.map(dict(Clas_Coef))\n",
    "df_clas = df_clas.iloc[:sum([len(x[2]) for x in nc[:(no_days+1)]])]\n",
    "zones = [int(z[3:]) for z in df_full.filter(regex = 'lz').columns]\n",
    "del df_full, cc\n",
    "\n",
    "# Load weather\n",
    "Weather_Scale = pd.read_csv('data/processed/MinMaxWeather.csv', index_col=0)\n",
    "\n",
    "\n",
    "def make_PTG(graph, zones, Weather_Scale):\n",
    "    attr, adj = graph\n",
    "\n",
    "    # Filter out \n",
    "    if (attr.time_to_reservation.values[-1] >= 48) or ~attr.next_customer[-1]:\n",
    "        return None\n",
    "    \n",
    "    if attr.leave_zone[-1] not in zones:\n",
    "        return None\n",
    "\n",
    "    # Slice\n",
    "    _, labels = sparse.csgraph.connected_components(csgraph=adj, directed=False, return_labels=True)\n",
    "    newl = labels[-1]\n",
    "    indices = labels == newl   \n",
    "\n",
    "    attr = attr[indices]\n",
    "    adj = adj[indices,:].tocsc()[:,indices].tocsr()\n",
    "\n",
    "    # drop\n",
    "    attr.drop(columns=['park_location_lat', 'park_location_long', 'leave_location_lat', 'leave_location_long', 'park_fuel', 'park_zone', 'moved', 'movedTF', 'prev_customer', 'next_customer', 'action'], inplace = True)\n",
    "    attr.drop(columns = ['leave_zone'], inplace = True)\n",
    "    # One hot encoding\n",
    "    attr['engine']= pd.Categorical(attr['engine'], categories=['118I', 'I3', 'COOPER', 'X1'])\n",
    "    attr = pd.get_dummies(attr, columns = ['engine'], prefix='eng')\n",
    "\n",
    "    # Add degree\n",
    "    attr['degree'] = np.squeeze(np.asarray(adj.sum(axis=1)))/50\n",
    "\n",
    "    # Normalize fuel, weahter and dist \n",
    "    attr['leave_fuel'] = attr['leave_fuel']/100\n",
    "    attr['dist_to_station'] = attr['dist_to_station']/5000\n",
    "    attr[Weather_Scale.index] = (attr[Weather_Scale.index] - Weather_Scale['Min'])/Weather_Scale['diff']\n",
    "\n",
    "    if attr.isnull().sum().sum() > 0:\n",
    "        return attr\n",
    "\n",
    "    # Get edges\n",
    "    edge_index, edge_weight = utils.convert.from_scipy_sparse_matrix(adj)\n",
    "\n",
    "    # Make pytorch data type\n",
    "    d = data.Data(x = torch.tensor(attr.drop(columns = ['time_to_reservation']).to_numpy(dtype = 'float')).float(), edge_index=edge_index, edge_attr=edge_weight.float(), y = torch.tensor(attr.time_to_reservation.values).float())\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "sdate = date(2019, 9, 1) # start date\n",
    "delta = timedelta(days=2)\n",
    "files = ['data/processed/Graphs/'+(sdate + timedelta(days=i)).strftime(\"%Y%m%d\")+'.pickle' for i in range(delta.days + 1)]\n",
    "\n",
    "dataset = []\n",
    "\n",
    "with open(files[0], 'rb') as f:\n",
    "    graph_collection = pickle.load(f)\n",
    "\n",
    "for g in graph_collection.values():\n",
    "    res = make_PTG(g,zones, Weather_Scale)\n",
    "    dataset.append(res)\n",
    "    if type(res) == type(df_clas):\n",
    "        print('hi')\n",
    "        break\n",
    "\n",
    "#train_data = [dataset[i] for i in nc[0][0]]\n",
    "#val_data = [dataset[i] for i in nc[0][1]]\n",
    "#test_data = [dataset[i] for i in nc[0][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:48<00:00, 24.39s/it]\n"
     ]
    }
   ],
   "source": [
    "for file, slicer in tqdm(zip(files[1:], nc[1:len(files)]), total = len(files)-1):\n",
    "    dataset = []\n",
    "    with open(file, 'rb') as f:\n",
    "        graph_collection = pickle.load(f)\n",
    "\n",
    "    for g in graph_collection.values():\n",
    "        res = make_PTG(g,zones, Weather_Scale)\n",
    "        if res:\n",
    "            dataset.append(res)\n",
    "\n",
    "    train_data = torch.utils.data.ConcatDataset([train_data,[dataset[i] for i in slicer[0]]])\n",
    "    val_data = torch.utils.data.ConcatDataset([val_data,[dataset[i] for i in slicer[1]]])\n",
    "    test_data = torch.utils.data.ConcatDataset([test_data,[dataset[i] for i in slicer[2]]])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True, num_workers = 4)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last=True, num_workers = 4)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False, drop_last=False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convM = Sequential('x, edge_index, edge_weight', [\n",
    "        (GCNConv(17,12, aggr = 'max'),'x, edge_index, edge_weight -> x'),\n",
    "        nn.ReLU(inplace = True),\n",
    "        (nn.Dropout(0.25), 'x -> x')\n",
    "        ])\n",
    "\n",
    "        self.convA = Sequential('x, edge_index, edge_weight', [\n",
    "        (GCNConv(17,12, aggr = 'add'),'x, edge_index, edge_weight -> x'),\n",
    "        nn.ReLU(inplace = True),\n",
    "        (nn.Dropout(0.2), 'x -> x')\n",
    "        ])\n",
    "\n",
    "        self.linS = Sequential('x', [\n",
    "        (Linear(17,12),'x -> x'),\n",
    "        nn.ReLU(inplace = True),\n",
    "        (nn.Dropout(0.2), 'x -> x')\n",
    "        ])\n",
    "\n",
    "        self.seq = Sequential('x', [\n",
    "            (Linear(36,16),'x -> x'),\n",
    "            nn.ReLU(inplace = True),\n",
    "            (nn.Dropout(0.2), 'x -> x'),\n",
    "            (Linear(16,1),'x -> x')\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        xConvM = self.convM(x, edge_index, edge_weight)\n",
    "        xConvA = self.convA(x, edge_index, edge_weight)\n",
    "        xLin = self.linS(x)\n",
    "\n",
    "        x = torch.cat([xConvM,xConvA,xLin], axis = 1)\n",
    "\n",
    "        x = self.seq(x)\n",
    "\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (convM): Sequential(\n",
      "    (0): GCNConv(17, 12)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (convA): Sequential(\n",
      "    (0): GCNConv(17, 12)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (linS): Sequential(\n",
      "    (0): Linear(17, 12, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (seq): Sequential(\n",
      "    (0): Linear(36, 16, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(16, 1, bias=True)\n",
      "  )\n",
      ") 1257\n",
      "Start learning\n"
     ]
    }
   ],
   "source": [
    "GNN = GCN().to(device)\n",
    "print(GNN, sum(p.numel() for p in GNN.parameters()))\n",
    "print('Start learning')\n",
    "\n",
    "optimizer = optim.Adam(GNN.parameters(), lr=0.001, weight_decay = 0.00001) #Chaged to Adam and learning + regulariztion rate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3z/pggbcw7949507j0lbw1gjngc0000gn/T/ipykernel_39239/2676899335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mtrain_r2_cur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_targs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mvalid_r2_cur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_targs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;34m-\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \"\"\"\n\u001b[0;32m--> 676\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    677\u001b[0m         y_true, y_pred, multioutput)\n\u001b[1;32m    678\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    721\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/GNN_env/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# Set number of epochs\n",
    "num_epochs = 2\n",
    "\n",
    "# Set up lists for loss/R2\n",
    "train_r2, train_loss = [], []\n",
    "valid_r2, valid_loss = [], []\n",
    "cur_loss = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "no_train = len(train_loader)\n",
    "no_val = len(val_loader)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    ### Train\n",
    "    cur_loss_train = 0\n",
    "    GNN.train()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = GNN(batch)\n",
    "        batch_loss = r2_loss(out[batch.ptr[1:]-1],batch.y[batch.ptr[1:]-1])\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cur_loss_train += batch_loss.item()\n",
    "    \n",
    "    train_losses.append(cur_loss_train/no_train)\n",
    "\n",
    "    ### Evaluate training\n",
    "    with torch.no_grad():\n",
    "        GNN.eval()\n",
    "        train_preds, train_targs = [], []\n",
    "        for batch in train_loader:\n",
    "            target_mask = batch.ptr[1:]-1\n",
    "            batch.to(device)\n",
    "            preds = GNN(batch)\n",
    "            train_targs += list(batch.y.cpu().numpy()[target_mask])\n",
    "            train_preds += list(preds.cpu().detach().numpy()[target_mask])\n",
    "\n",
    "\n",
    "    ### Evaluate validation\n",
    "        val_preds, val_targs = [], []\n",
    "        cur_loss_val = 0\n",
    "        for batch in val_loader:\n",
    "            batch.to(device)\n",
    "            preds = GNN(batch)[batch.ptr[1:]-1]\n",
    "            y_val = batch.y[batch.ptr[1:]-1]\n",
    "            val_targs += list(y_val.cpu().numpy())\n",
    "            val_preds += list(preds.cpu().detach().numpy())\n",
    "            cur_loss_val += r2_loss(preds, y_val)\n",
    "\n",
    "        val_losses.append(cur_loss_val/no_val)\n",
    "\n",
    "\n",
    "    train_r2_cur = r2_score(train_targs, train_preds)\n",
    "    valid_r2_cur = r2_score(val_targs, val_preds)\n",
    "    \n",
    "    train_r2.append(train_r2_cur)\n",
    "    valid_r2.append(valid_r2_cur)\n",
    "\n",
    "    print(\"Epoch %2i: Train Loss %f, Valid Loss %f, Train R2 %f, Valid R2 %f\" % (\n",
    "                epoch+1, train_losses[-1], val_losses[-1],train_r2_cur, valid_r2_cur))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0200, 0.0000, 0.0000, 0.0600, 0.0000,    nan, 0.0200])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.x[:7,-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(121)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(batch.x[:,-7]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(GNN(batch)[batch.ptr[1:]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8400,  1.0000, -0.3006,  ...,  0.0000,  0.0000,  0.0901],\n",
       "        [ 0.1500,  1.0000,  0.6988,  ...,  0.0000,  0.0000,  0.1655],\n",
       "        [ 0.7300,  1.0000,  0.2284,  ...,  0.0000,  0.0000,  0.3286],\n",
       "        ...,\n",
       "        [ 0.6500,  1.0000,  0.8953,  ...,  0.0000,  0.0000,  0.2812],\n",
       "        [ 0.5600,  1.0000,  0.8922,  ...,  0.0000,  0.0000,  0.3438],\n",
       "        [ 0.9700,  1.0000,  0.8898,  ...,  0.0000,  0.0000,  0.0410]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[1].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "#from torch_geometric import utils, data\n",
    " \n",
    "def haversine_start(df, car1, car2, max_dist = 1500):\n",
    "    def _edge_weight(x, max_dist):\n",
    "        return max((max_dist-x)/max_dist,0)\n",
    "    dfc1 = df[df.car == car1]\n",
    "    dfc2 = df[df.car == car2]\n",
    "\n",
    "    point1 = dfc1[['leave_location_lat','leave_location_long']].values[0]\n",
    "    point2 = dfc2[['leave_location_lat','leave_location_long']].values[0]\n",
    "    #return point1\n",
    "\n",
    "    # convert decimal degrees to radians\n",
    "    lat1, lon1 = map(np.radians, point1)\n",
    "    lat2, lon2 = map(np.radians, point2)\n",
    "\n",
    "    # Deltas\n",
    "    delta_lon = lon2 - lon1 \n",
    "    delta_lat = lat2 - lat1 \n",
    "    \n",
    "    # haversine formula \n",
    "    a = np.sin(delta_lat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(delta_lon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371000 # Radius of earth in m\n",
    "    return _edge_weight(c * r, max_dist)\n",
    "\n",
    "def haversine_add(current_locs, to_add, max_dist = 1500):\n",
    "    def _edge_weight(x, max_dist):\n",
    "        return max((max_dist-x)/max_dist,0)\n",
    "    def _haversine(point1, lat2, lon2):\n",
    "        \n",
    "         # convert decimal degrees to radians\n",
    "        lat1, lon1 = map(np.radians, point1)\n",
    "\n",
    "        # Deltas\n",
    "        delta_lon = lon2 - lon1 \n",
    "        delta_lat = lat2 - lat1 \n",
    "        \n",
    "        # haversine formula \n",
    "        a = np.sin(delta_lat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(delta_lon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a)) \n",
    "        r = 6371000 # Radius of earth in m\n",
    "        return c * r\n",
    "\n",
    "    leave_lat_add, leave_long_add = to_add.leave_location_lat, to_add.leave_location_long\n",
    "\n",
    "    lat2, lon2 = map(np.radians, [leave_lat_add,leave_long_add])\n",
    "\n",
    "    new_weights = [_edge_weight(_haversine(loc, lat2, lon2), max_dist) for loc in current_locs]\n",
    "\n",
    "    return new_weights\n",
    "\n",
    "def delete_rc(mat, i):\n",
    "    # row\n",
    "    n = mat.indptr[i+1] - mat.indptr[i]\n",
    "    if n > 0:\n",
    "        mat.data[mat.indptr[i]:-n] = mat.data[mat.indptr[i+1]:]\n",
    "        mat.data = mat.data[:-n]\n",
    "        mat.indices[mat.indptr[i]:-n] = mat.indices[mat.indptr[i+1]:]\n",
    "        mat.indices = mat.indices[:-n]\n",
    "    mat.indptr[i:-1] = mat.indptr[i+1:]\n",
    "    mat.indptr[i:] -= n\n",
    "    mat.indptr = mat.indptr[:-1]\n",
    "    mat._shape = (mat._shape[0]-1, mat._shape[1])\n",
    "\n",
    "    # col\n",
    "    mat = mat.tocsc()\n",
    "    n = mat.indptr[i+1] - mat.indptr[i]\n",
    "    if n > 0:\n",
    "        mat.data[mat.indptr[i]:-n] = mat.data[mat.indptr[i+1]:]\n",
    "        mat.data = mat.data[:-n]\n",
    "        mat.indices[mat.indptr[i]:-n] = mat.indices[mat.indptr[i+1]:]\n",
    "        mat.indices = mat.indices[:-n]\n",
    "    mat.indptr[i:-1] = mat.indptr[i+1:]\n",
    "    mat.indptr[i:] -= n\n",
    "    mat.indptr = mat.indptr[:-1]\n",
    "    mat._shape = (mat._shape[0], mat._shape[1]-1)\n",
    "\n",
    "    return mat.tocsr()\n",
    "\n",
    "# Haversine function for stations\n",
    "def haversine_station(point1, point2):\n",
    "    # convert decimal degrees to radians\n",
    "    lat1, lon1 = map(np.radians, point1)\n",
    "    lat2, lon2 = map(np.radians, point2)\n",
    "\n",
    "    # Deltas\n",
    "    delta_lon = lon2 - lon1 \n",
    "    delta_lat = lat2 - lat1 \n",
    "\n",
    "    # haversine formula \n",
    "    a = np.sin(delta_lat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(delta_lon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371000 # Radius of earth in m\n",
    "    return c * r\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/processed/VacancySplit.csv', index_col=0, parse_dates = [2]).astype({'time_to_reservation': 'float32', 'park_location_lat': 'float32', 'park_location_long': 'float32', 'leave_location_lat': 'float32', 'leave_location_long': 'float32', 'park_zone': 'int32', 'leave_zone': 'int32', 'park_fuel': 'int8', 'leave_fuel': 'int8', 'moved': 'float32', 'movedTF': 'bool'})\n",
    "\n",
    "# Time variables\n",
    "df['weekend'] = df.time.dt.weekday//5\n",
    "\n",
    "# Time encoding\n",
    "def circle_transform(col, max_val=86400):\n",
    "    tot_sec = ((col - col.dt.normalize()) / pd.Timedelta('1 second')).astype(int)\n",
    "    cos_val = np.cos(2*np.pi*tot_sec/max_val)\n",
    "    sin_val = np.sin(2*np.pi*tot_sec/max_val)\n",
    "    return cos_val, sin_val\n",
    "\n",
    "df['Time_Cos'], df['Time_Sin'] = [x.values for x in circle_transform(df.time)]\n",
    "\n",
    "# Join weather\n",
    "df_weather = pd.read_csv('data/processed/weather.csv', index_col=0, parse_dates=[0])\n",
    "df['timeH'] = df.time.round('H')\n",
    "df = df.set_index('timeH').join(df_weather).reset_index(drop=True)\n",
    "\n",
    "# Create init graph\n",
    "Start_Time = pd.Timestamp('2019-08-31 21:00:00')\n",
    "End_Time = pd.Timestamp('2019-11-01 12:00:00')\n",
    "start_df = df[df.time <= Start_Time]\n",
    "propegate_df = df[(df.time > Start_Time) & (df.time <= End_Time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "CarID_dict_start = dict(iter(start_df.groupby('car')))\n",
    "Start_Garph_data = []\n",
    "\n",
    "for sub_df in CarID_dict_start.values():\n",
    "    last_obs = sub_df.iloc[-1]\n",
    "    if last_obs.action: # True is park\n",
    "        Start_Garph_data.append(last_obs)\n",
    "\n",
    "start_df_graph = pd.DataFrame(Start_Garph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5509f31e2044e4d39f528bdd5e1bbe95ee9581822c64151f31115901a677525b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('GNN_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
